% #############################################################################
% This is Chapter 6
% !TEX root = ../main.tex
% #############################################################################
% Change the Name of the Chapter i the following line
\fancychapter{Evaluation}
% The following line allows to ref this chapter
\label{chap:evaluation}

After completing the last cycle of development, a set of users tested the Sterio system, in order to gather quantitative and qualitative usability metrics to assure that our platform meets both users’ needs and our set goals. We start by presenting the methodology used, followed by the description of the tasks defined for the test sessions, justifying, for each, what we want to conclude by asking users to do it. We finally present the analysis of the test results and the workload estimated for the prototype, as well as the conclusions that we were able to get from the results.


% #############################################################################
\section{Methodology}

When a final functional prototype of the platform with a working set of features was completed, a group of 26 potential users tested the system. This set of users were of distinct ages, occupations, socio-economical backgrounds, and audio media consuming habits. From these users, 21 haven't participated in either of the previously-mentioned user research (described in Chapter ~\ref{chap:userresearch}) and speed dating activities (examined in Chapter ~\ref{sec:speeddating}), while the remaining 5 have participated in these ventures.

This evaluation was conducted to assess the success of the final prototype and to check that a standard was upheld, which is a process known as summative evaluation. ~\cite{Preece2015, Courage2005} The same list of tasks and protocols were presented to each user, and their performance was evaluated mainly through qualitative measures, as we want to deeply understand the type of experience that is created while users indulge in the platform, as well as insights, findings, and anecdotes about the experience of the user.

To help us steer the session, and to keep all gatherings as cohesive and alike as possible, the first step was to write a protocol guide, shown in Appendix CENAS. All sessions were conducted in a physical location, where several measures were taken to comply with health and safety guidelines as a response to the COVID-19 pandemic. For instance, the researcher and the user were seated at least 2 meters apart from each other, complying with the social distancing rules. All surfaces — including the provided smartphone on where the prototype was tested — were disinfected before and after the session. Users were required to utilize hand sanitizer when entering and leaving the room and were also asked to bring their smartphone so that they could fill out the necessary survey forms. 

We planned each testing session to be divided into 3 distinct segments, which we will describe in the following subsections.

\subsection{Introduction, Informed Consent Form, and Initial Survey}

After the user's arrival to the testing room, the facilitator invited them to sit in a comfortable way. In front of them, three items were displayed: a smartphone with the loaded Sterio system; a sheet containing a set of QR codes that redirects users to the necessary survey forms; and a helping sheet that contains extra information regarding the tasks.

In order to contextualize each user on what the purposes of the testing were, an introduction was read by the research. Then, users were asked to carefully read and sign an informed consent form (presented in Appendix CENAS. Finally, by presenting them an initial survey (showcased in Appendix CENAS), we collected demographic information and other relevant details of the user, such as if they had any visual or hearing conditions, as well as their general audio media consumption habits.

\subsection{User Training and Task Protocol}
\label{subsub:taskprot}

After the initial remarks, the user was allowed a maximum of five minutes to explore freely the platform's four main screens. The remaining screens were not available for the users to explore in the first stage, as this could interfere with the testing results. During this period, the user could ask any questions. After they felt ready to do so, we began the testing session.

The core testing session consisted of 4 different tasks, that are further described in Section ~\ref{sub: tasks}. Each task followed a specific protocol that was transversal to all tasks. First, the researcher presented the task and gave space for the user to clarify any questions related to the disclosure of the task.

Then, after the consent of the user, the researcher started a stopwatch timer to count the time the user took to perform the task. Furthermore, the screen of the used smartphone was also recorded, to help later in the protocol. At the same time, the facilitator was paying attention to the user's actions, taking relevant notes about the usability when appropriate, and counting the number of errors (if any occurred). Beforehand, it was communicated to the user that it was not possible to express any comments nor ask any questions unless a very high level of difficulty whilst performing the task was detected.

Right after the conclusion of the task, the user is asked to fill out a post-task survey that evaluates quantitatively the general experience, usability, and difficulties felt by the user. This survey is showcased in Appendix CENAS.

To gather a broad dataset of qualitative data, two types of moderation to encourage each tester to share their thought process were applied: \ac{RTA}, where the moderator asks participants to retrace their steps when the session is complete, and \ac{RP}, where the researcher asks detailed and relevant questions after the fact.

Regarding \ac{RTA}, a video replay of the user's actions was shown, so that it was easier for them to recall and express their line of thought as they performed the task. The researcher took relevant notes as the user expressed their reasoning.

Lastly, users were asked specific questions about their thoughts and action, such as "What would you do differently?" and were encouraged to elaborate on their responses. As the user was expressing comments, the researcher took relevant notes.

Each of the four tasks followed this protocol, whose duration was an average of 5 minutes per item. 

\subsection{Final Debrief}

After the conclusion of the four tasks, users were redirected to a final survey, presented in Appendix CENAS, which was subdivided into two sections. 

The first half consisted of a \ac{SUS}, which is a simple, ten-item scale giving a global view of subjective assessments of usability [28] about the user experience with the Sterio system. We followed the guidelines established by Brooke [28]: each question had a degree of disagreement or agreement, with a range from Strongly Disagree (1) to Strongly Agree (5) respectively, from which the user could choose. Users were asked to answer each question honestly, but not too attentively.

The latter half of the final survey consisted of Microsoft's Product Reaction Cards method, which consists of a list of 118 words that might be used to describe a product. The list includes positive words like ‘Useful’ and ‘Engaging’, together with negative words, such as ‘Frustrating’ and ‘Ineffective’. Users were asked to choose up to 5 of these words, which were sorted randomly to avoid any bias.

Finally, to close the session, a short final interview with the user was conducted. These interviews allowed the participants to shed light on their experience without extra prompting. A semi-structured approach using a few predetermined questions was applied in the first stage, but afterward, the interviews took their own direction, which uncovered some very useful insights regarding our platform. As the interview was unrolled, the researcher took note of relevant aspects and observations.

% #############################################################################
\section{Tasks}
~\label{sub:tasks}

The core evaluation session consisted of 4 different tasks that allowed us to understand if our platform met our established usability goals. These tasks should not be too complex, but should be able to explore the full capabilities and features of our prototype, so that we can uncover as much detail as possible regarding the user's experience.

The complete set of four tasks is:

\begin{enumerate}
	\item Create a new station \textit{(Create)}
	\item Configure the station's blocks \textit{(Create)}
	\item Play the created station \textit{(Listen)}
	\item Share the created station \textit{(Share)}
\end{enumerate}

These tasks are focused on the defined three main user enactments on Section ~\ref{sec:userenactments} — tasks 1 and 2 for the 'Create' enactment, task 3 for the 'Listen' enactment, and task 4 for the 'Share' enactment. This setting allowed us to better understand and organize the session, as well as to steer our results and compare them with the previously discussed matter.

In the first task, users were asked to create a new station with a given name ('Feel Good'), description ('The best hits!'), cover (the first image on the gallery of the smartphone), and blocks (Spotify, Weather, and News). This was a simple task that evaluated the ease of use of the platform, as well as how quickly the user can create a fully tailored and customized station.

The second task was the most complex, as it required a lot of input from the user. Users were asked to configure the three added blocks (Spotify, Weather, and News). For the Spotify block, they were asked to select one of the 5 available playlists, which had identical duration but with a distinct set of songs to match the user's taste. In the Weather block, users were asked to select the current location, current conditions, hourly forecast, and 3-day forecast, with the periodicity set to 5 minutes. Finally, in the News block, users were asked to select the 'General', 'Health', and 'Entertainment' categories, with 6 as the number of headlines and periodicity of 5 minutes. One of our established goals was to make as simple as possible for the user to tailor and customize the station to their taste, and this task let us uncover helpful insights in this matter.

The third task was the most simple one for the user to perform but was the most critical for our study. Users were asked to play their created station and to listen carefully to its content. Then, they were asked to enter in the 'Schedule' screen of the station, as well as to find and enter the 'Now Playing' screen whilst the station was playing. Ultimately, this task gave us really important feedback on the experiences the users felt while indulging in this new listening model.

Finally, the fourth task tested our platform's social capabilities. Users were asked to enter in the 'Social' screen and follow the 'Roger Waters' profile. Then, it was simulated that such a profile was listening to a shared station, and users were asked to listen along (testing the simultaneous listening experience). Afterward, users were asked to enter the "My Day" shared station, and change the News periodicity to 4 minutes. This task allowed users to experience the social counterpart of the platform, giving us important feedback on their experience.

The execution of each of the four tasks followed the same protocol, described previously in Section ~\ref{subsub:taskprot}. Each task didn't surpass 5 minutes in performing them, which allowed us to maintain our goal of keeping the total duration of the sessions in the window of 30 to 35 minutes.

% #############################################################################
\section{Results}

% #############################################################################
\section{Discussion}